---
title: "kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection"
author: "Lotfi Slim"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<style>
body {
text-align: justify}
</style>

In this vignette, we illustrate on a synthetic dataset how to perform post-selection inference for a set of kernels using our R package **kernelPSI**. The kernels are selected in a forward fashion according to quadratic kernel association scores, leading to the modelisation of the selection event as a set of quadratic constraints. For valid inferene, we need to correct for the bias introduced by the prior selection of the kernels. Determining the exact post-selection distribution of our test statistics under the selection event was impossible. To overcome that, we use sampling in order to derive empirical $p$-values. 

## Simulation

We first start by giving the setup for our simulation. We associate $10$ gaussian kernels to $10$ independent groups of variables of size $5$ each. Within each group, the variables are drawn from a multivariate normal distribution with mean $0$ and a correlation matrix $V_{ij} = \rho^{\lvert i-j\rvert}$, where $\rho = 0.6$ and $i,j\in[1\mathrel{{.}\,{.}} 5]$. The dataset we consider here consists of $100$ independent samples. 

```{r}
require("MASS")

n_kernels <- 10
size_kernels <- 5

n <- 100
rho <- 0.6
corr <- outer(seq_len(size_kernels), seq_len(size_kernels),
              function(i, j) return(rho^(abs(i-j))))
X <- replicate(n_kernels,
               mvrnorm(n, mu = rep(0, size_kernels), Sigma = corr),
               simplify = FALSE)
```

The kernels gaussian. We use centerd kernels
```{r}
require("kernlab")
K <- replicate(n_kernels, rbfdot(sigma = 1 / size_kernels))
Kmat <- sapply(seq_len(n_kernels),
               function(i) {kMatrix <- kernelMatrix(K[[i]], X[[i]]); 
               return(as.kernelMatrix(kMatrix, center = TRUE))},
               simplify = FALSE)
```

simulation model
```{r}
m_kernels <- 3
theta <- 0.1

Ksum <- Reduce(`+`, Kmat[seq_len(m_kernels)])
decompK <- eigen(Ksum)

Y <- as.matrix(theta * decompK$values[1] * decompK$vectors[, 1] + rnorm(n), ncol = 1)
Lmat <- kernelMatrix(new("vanillakernel"), Y)
```

## Kernel selection
```{r}
require("kernelPSI")

candidate_kernels <- 3
selectFOHSIC <- FOHSIC(Kmat, Lmat, mKernels = candidate_kernels)
constraintFO <- forwardQ(Kmat, selectFOHSIC)
```

```{r}
selectAHSIC <- adaFOHSIC(Kmat, Lmat)
adaFO <- adaQ(Kmat, selectAHSIC[["selection"]], selectAHSIC[["n"]])
adaS <- selectAHSIC$selection[seq_len(selectAHSIC$n)] # indices of selected kernels
```

## Inference
```{r}
n_replicates <- 5000
burn_in <- 1000

kernelPSI(Y, K_select = Kmat[selectFOHSIC], constraintFO, 
          n_replicates = n_replicates, burn_in = burn_in)
```

```{r}
kernelPSI(Y, K_select = Kmat[adaS], constraintFO, 
          n_replicates = n_replicates, burn_in = burn_in)
```

